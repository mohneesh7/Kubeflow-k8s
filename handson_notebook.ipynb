{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOPS Kubernetes and Kubeflow Hands On Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubernetes Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Containers and Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Build a simple Docker image for a Python application and run it locally.\n",
    "---\n",
    "**Solution:** \n",
    "* Simple FastAPI web application ---> can be found in the `./webapp/` path\n",
    "* Dockerfile to build the image ---> can be found at `./Dockerfile`\n",
    "* Commands to run the fastAPI application using Docker.\n",
    "    ```\n",
    "    $ docker build -t fastapi_sample_app .\n",
    "    $ docker run -p 8080:8080 fastapi_sample_app\n",
    "    ```\n",
    "* Open `localhost:8080` in the browser to access the application (or use postman to test the API)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro. to Kubernetes : Architecture and Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Explore a running Kubernetes cluster using kubectl (get nodes, pods)\n",
    "----\n",
    "* `kubectl get pods`\n",
    "\n",
    "![get pods](images/pods.png)\n",
    "\n",
    "* `kubectl get svc`\n",
    "\n",
    "![get svc](images/svc.png)\n",
    "\n",
    "* `kubectl get nodes`\n",
    "* `kubectl get deployments`\n",
    "\n",
    "etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up kubernetes cluster (Minikube/Kind) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Install Minikube and create a local kubernetes cluster\n",
    "---\n",
    "\n",
    "To install Minikube on Windows (WSL2):\n",
    "```\n",
    "   > curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\n",
    "\n",
    "   > chmod +x ./minikube\n",
    "   > sudo mv ./minikube /usr/local/bin/\n",
    "   > minikube config set driver docker\n",
    "\n",
    "```\n",
    "Next install Kubectl and use minikube kubectl as default kubectl\n",
    "```\n",
    "   > curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n",
    "   > chmod +x ./kubectl\n",
    "   > sudo mv ./kubectl /usr/local/bin/\n",
    "   > kubectl config use-context minikube\n",
    "   > alias kubectl=\"minikube kubectl --\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on: Deploy your first Application on Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Create a simple web app and deploy it as K8s deployment.\n",
    "---\n",
    "**Solution:**\n",
    "1. In my case I am using WSL2 inside windows, hence I need to point minikube to the correct docker engine inside minikube.\n",
    "2. Run below command in terminal (Command can be found in minikube docs)\n",
    "```\n",
    "    > eval $(minikube -p minikube docker-env)\n",
    "```\n",
    "3. Build the Docker image again (alternatively we can pull the local built image from the local registry, but for simplicity I will just build this image again inside minikube). Also giving a tag to the image is important as it causes issues when creating deployment.\n",
    "4. Run below command in terminal (Command can be found in minikube docs)\n",
    "```\n",
    "    > docker build -t sample_fast_api:v2 .\n",
    "```\n",
    "5. Create the deployment\n",
    "```\n",
    "    > kubectl create deployment app-dep --image=sample_fast_api:v2\n",
    "```\n",
    "6. Now the application is ready and you can check the deployment my using the `kubectl get pods` command to view the pod exactly. or use `kubectl get deploy` to view the deployments.\n",
    "\n",
    "\n",
    "---\n",
    "#### Task 2: Expose the application using NodePort or LoadBalancer service. \n",
    "---\n",
    "**Solution:**\n",
    "1. Now tht we have a running application, we need to expose the application using NodePort or LoadBalancer service. For this I will use the NodePort service.\n",
    "2. Run below command in terminal (Command can be found in minikube docs)\n",
    "```\n",
    "    > kubectl expose deployment app-dep --type=NodePort --port=8080\n",
    "```\n",
    "3. Now the service is created, to test this locally using minikube you can expose it using the below commnad:\n",
    "```\n",
    "    > minikube service app-dep\n",
    "```\n",
    "4. We should see something like this:\n",
    "\n",
    "    ![minikube service](images/minikube_service.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubernetes Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pods, Deployments and Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Hands-on: Create a deployment with multiple replicas of a pod.\n",
    "---\n",
    "\n",
    "**Solution:**\n",
    "1. I have create a deployment and a service in a single file ---> `./app-dep-deployment`\n",
    "2. This will have 2 replicas, and created a nodeport serive on the port 30000 mapping the container port of 8080.\n",
    "3. create the deplyment using `kubectl create -f app-dep-deployment`\n",
    "4. Because I am using docker desktop's engine here inside WSL2 we need to run the `minikube service app-dep-service` command to expose the service to test locally.\n",
    "\n",
    "#### Task 2: Expose the deployment as a service and access it.\n",
    "---\n",
    "\n",
    "**Solution:**\n",
    "1. This is already achieved in the previous step, we will see something like this:\n",
    "\n",
    "    ![minikube service](images/service_nodeport.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConfigMaps and Secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Create a Configmap to store application configuration and a secret to store sensitive information.\n",
    "---\n",
    "**Solution:**\n",
    "1. I have created a new YAML file name `app-dep-secret-configmap,yaml`, this contains a secret and a configmap. The deployment will be similar to the previous task, this time I have rebuilt the system with a new docker build as I have made a few changes to the fastAPI application to verify if we are able to access these values as environment variables.\n",
    "\n",
    "    ![newly added settings](images/secret_configmap.png)\n",
    "2. Run `kubectl create -f app-dep-secret-configmap.yaml`\n",
    "3. As we are in WSL2 run `minikube service app-dep-service`\n",
    "\n",
    "  \n",
    "---\n",
    "\n",
    "#### Task 2: Mount ConfigMap and Secret as environment variable in the pod.\n",
    "---\n",
    "4. If we hit the `/secret` and `/configMap` endpoints, we should see the values  in the response. These were set as env variables in the deployment.\n",
    "  ![configmap output](images/configMap.png)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestration using Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Kubeflow on k8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task : Install kubeflow on minikube\n",
    "---\n",
    "\n",
    "**Solution:**\n",
    "1. First we need to install a dependancy called Kustomize so that we can install all the components of kubeflow.\n",
    "2. Run below command in terminal:\n",
    "```\n",
    "    > curl -s \"<https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh>\"  | bash\n",
    "    > sudo install kustomize /usr/local/bin/kustomize\n",
    "    > git clone <https://github.com/kubeflow/manifests.git>\n",
    "    > cd manifests\n",
    "    > while ! kustomize build example | awk '!/well-defined/' | kubectl apply -f -; do echo \"Retrying to apply resources\"; sleep 10; done\n",
    "    > kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\n",
    "```\n",
    "**Note:** Keep at least 8GB ram and 8 CPU to the minikube cluster for it to work and get installed.\n",
    "**Note:** The last command is necessary to port forward the dashboard of kubeflow.\n",
    "\n",
    "If the installation is not happening then use this link to install each component individually: https://github.com/kubeflow/manifests#install-individual-components\n",
    "\n",
    "After Successful installation of kubeflow components, you can access the dashboard of kubeflow by opening the following link: http://localhost:8080, It should look something like this:\n",
    "\n",
    "![kubeflowdash](images/kubeflow_dash.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Kubeflow Pipelines UI overview and its components\n",
    "---\n",
    "1. Pipelines lists all the installed/added pipelines. (These can be aded through a YAML file or python SDK)\n",
    "![pipelines](images/pipeline.png)\n",
    "\n",
    "2. Experiment: This stores all the runs and organizes them under experiment names/ID's\n",
    "![experiment](images/experiment.png)\n",
    "\n",
    "3. We can get the details of the pipeline in a visual manner when we click on the corresponding run.\n",
    "![pipeline_details](images/kfp_run_dag.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Managing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task : Build a custom ML pipeline and visualize its execution.\n",
    "---\n",
    "1. You can find the pipeline code at `webapp/kubeflow_code/kubeflow_pipeline.py`. \n",
    "2. After compiling the pipeline, you will the YAML file at `webapp/kubeflow_code/kubeflow_iris_classifier.yaml`. \n",
    "3. Adding the YAML to the kubeflow pipeline UI and create a run.\n",
    "4. The run could be triggered from the pipeline UI or set a recurring run.\n",
    "5. In the pipeline we have multiple components which will run one after the other as configured.\n",
    "6. The pipeline will then run step by step which could be seen visually like below.\n",
    "![kfp_dag](images/kfp_run_dag.png)\n",
    "7. After the run is completed we can see the metrics of the run in the logs section of the get_metrics step:\n",
    "![kfp_metrics](images/metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Training and AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Training with Kubeflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Setup Distributed training job with Kubeflow\n",
    "---\n",
    "1. To setup distributed training on kubeflow, we need to install Training Operator.\n",
    "```\n",
    "> kubectl apply -k \"github.com/kubeflow/training-operator.git/manifests/overlays/standalone?ref=v1.8.1\"\n",
    "\n",
    "> kubectl apply -k \"github.com/kubeflow/training-operator.git/manifests/overlays/standalone?ref=master\"\n",
    "\n",
    "> pip install -U kubeflow-training \n",
    "> pip install git+https://github.com/kubeflow/training-operator.git@master#subdirectory=sdk/python\n",
    "\n",
    "```\n",
    "This will install the Training Operator and Python SDK for Kubeflow. I am doing indiviual component installs because of lack of memory and compute.\n",
    "\n",
    "2. Craete a training job, for an eample I have created a Xgboost training job that can be found at `webapp/training_operator/xgboost.yaml`. This will create 1 master and 2 worker node replicas. Which will do the training process.\n",
    "\n",
    "![dist_train](images/dist_train.png)\n",
    "\n",
    "#### Task: Compare the performance of the distributed training job with the single node training job\n",
    "---\n",
    "\n",
    "1. This completes the classification in 130 seconds as opposed to around 135 seconds it took for the single node training.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; position: relative;\">\n",
    "    <div style=\"width: 48%; position: relative;\">\n",
    "        <img src=\"images/dist_train.png\" alt=\"dist_train\" style=\"width: 100%;\">\n",
    "    </div>\n",
    "    <div style=\"width: 48%; position: relative;\">\n",
    "        <img src=\"images/test_train_single1.png\" alt=\"dist_train\" style=\"width: 100%; height\">\n",
    "    </div>\n",
    "    <div style=\"width: 48%; position: relative;\">\n",
    "        <img src=\"images/train_single.png\" alt=\"dist_train\" style=\"width: 100%;\">\n",
    "    </div>\n",
    "</div>\n",
    "<div style=\"text-align: center; font-size: 0.8em; margin-top: 10px;\">\n",
    "    Left: Distributed Job.      Right: Single Node Test Train Split and Train Step\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
